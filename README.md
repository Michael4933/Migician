# <img src="figs/logo.png" style="width: 20%"> Migician: Revealing the Magic of Free-Form Multi-Image Grounding in Multimodal Large Language Models
[You Li](https://openreview.net/profile?id=~You_Li9), [Heyu Huang](https://openreview.net/profile?id=~Heyu_Huang2)*, [Chen Chi](https://openreview.net/profile?id=~Chi_Chen1), [Kaiyu Huang](https://openreview.net/profile?id=~Kaiyu_Huang1), Chao Huang, Zonghao Guo, Zhiyuan Liu, Jinan Xu, Yuhua Li, Ruixuan Li, Maosong Sun

-----

<a href='https://michael4933.github.io/'><img src='https://img.shields.io/badge/Project-Page-Green'></a>
<a href='#'><img src='https://img.shields.io/badge/Demo-Page-purple'></a> 
<a href='https://arxiv.org/abs/2411.03628'><img src='https://img.shields.io/badge/Paper-PDF-orange'></a> 
<a href='https://huggingface.co/datasets/Michael4933/MIG-Bench'><img src='https://img.shields.io/badge/Benchmark-Huggingface-yellow'></a> 
<a href='https://huggingface.co/datasets/Michael4933/MGrounding-630k'><img src='https://img.shields.io/badge/Dataset-Huggingface-blue'></a> 

This repository hosts the data and benchmark utilization, training implementation, and model weight of Migician, the first competitive Multi-image Grounding MLLM

-----------

## ðŸ“° News
* **[2024.02.16]**  ðŸ‘€ðŸ‘€ðŸ‘€ Our [paper](https://arxiv.org/abs/2411.03628) has been accepted by ACL2025 as a Oral Paper!
* **[2025.01.09]**  ðŸ”¥ðŸ”¥ðŸ”¥ We have further released our multi-image grounding training dataset [MGrounding_630k](https://huggingface.co/datasets/Michael4933/MGrounding-630k) and our comprehensive multi-image grounding benchmark [MIG-Bench](https://huggingface.co/datasets/Michael4933/MIG-Bench) on HuggingfaceðŸ¤—~ Feel free to download and apply for your own use.
* **[2025.01.05]**  ðŸ’ªðŸ’ªðŸ’ª The model weight is now available on HuggingFace! ðŸ¤— [Huggingface Model](https://huggingface.co/Michael4933/Migician) Come and have a try!
* **[2025.01.02]** ðŸ’¥ðŸ’¥ðŸ’¥ We have released our paper on Arxiv at the start of the new year!
